{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "870a1090-f4ae-45a0-aa93-0960f5c6478d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db3e2fd1-db86-4bb6-a041-9507752f8264",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Pipeline params:**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X': 'something',\n",
      " 'env_name': 'user',\n",
      " 'pipeline_name': 'pipeline',\n",
      " 'zone_name': 'zone'}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Step params:**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Y': 'something_else'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load pipeline and step parameters - do not edit\n",
    "from sinara.substep import get_pipeline_params, get_step_params\n",
    "pipeline_params = get_pipeline_params(pprint=True)\n",
    "step_params = get_step_params(pprint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "453c78e8-6822-4123-bf9d-ed1ae9dd7bea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Specify sub_step parameters\n",
    "substep_params={\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3818420-6085-43ea-b97e-578742794e1e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**STEP NAME:**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'sinara_quick_test'\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**OUTPUTS:**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'user.pipeline.zone.sinara_quick_test.stored_files': '/data/home/jovyan/pipeline/zone/sinara_quick_test/run-24-06-11-113955/stored_files'}]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**TMP ENTITIES:**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'tmp:user.pipeline.zone.sinara_quick_test.tmp_dir_to_store': '/tmp/env/user/pipeline/zone/sinara_quick_test/run-24-06-11-113955/tmp_dir_to_store'},\n",
      " {'tmp:user.pipeline.zone.sinara_quick_test.tmp_dir_to_load': '/tmp/env/user/pipeline/zone/sinara_quick_test/run-24-06-11-113955/tmp_dir_to_load'}]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define substep interface\n",
    "from sinara.substep import NotebookSubstep, ENV_NAME, PIPELINE_NAME, ZONE_NAME, STEP_NAME, RUN_ID, ENTITY_NAME, ENTITY_PATH, SUBSTEP_NAME\n",
    "\n",
    "substep = NotebookSubstep(pipeline_params, step_params, substep_params)\n",
    "\n",
    "substep.interface(\n",
    "    tmp_entities =\n",
    "    [\n",
    "        { ENTITY_NAME: \"tmp_dir_to_store\" },\n",
    "        { ENTITY_NAME: \"tmp_dir_to_load\" }\n",
    "    ],\n",
    "    \n",
    "    # custom_inputs = \n",
    "    # [\n",
    "    #     { ENTITY_NAME: \"big_file\", ENTITY_PATH: \"/data/tmp/user/pipeline/zone/big_file\" }\n",
    "    # ],\n",
    "    \n",
    "    outputs = \n",
    "    [\n",
    "        { ENTITY_NAME: \"stored_files\" }\n",
    "    ]\n",
    ")\n",
    "\n",
    "substep.print_interface_info()\n",
    "\n",
    "substep.exit_in_visualize_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2048b57a-edb8-449d-bd76-b1bf41d8ef2f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session is run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/11 11:39:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='/proxy/4040/jobs/' target='blank'>Open Spark UI</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sinara.spark import SinaraSpark\n",
    "\n",
    "spark = SinaraSpark.run_session(0)\n",
    "SinaraSpark.ui_url()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "948a4010-0c06-4255-a7c6-482eea33cfa5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zlib\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "def crc32(fileName):\n",
    "    with open(fileName, 'rb') as fh:\n",
    "        hash = 0\n",
    "        while True:\n",
    "            s = fh.read(65536)\n",
    "            if not s:\n",
    "                break\n",
    "            hash = zlib.crc32(s, hash)\n",
    "        return \"%08X\" % (hash & 0xFFFFFFFF)\n",
    "\n",
    "def rm_rf(folder):\n",
    "    for filename in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac044d2e-aeb4-4e28-b980-f2a115d0ea2b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create test data\n",
    "# import os, random, string\n",
    "# from pathlib import Path\n",
    "# tmp_entities = substep.tmp_entities()\n",
    "\n",
    "\n",
    "# with open(f'{tmp_entities.tmp_dir_to_store}/big_file.bin', 'wb') as f:\n",
    "#     f.write(os.urandom(1000000))    # generate random content file larger than ROW_SIZE\n",
    "# crc1 = crc32(f'{tmp_entities.tmp_dir_to_store}/big_file.bin')\n",
    "\n",
    "# Path(tmp_entities.tmp_dir_to_store, \"subdir\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# with open(f'{tmp_entities.tmp_dir_to_store}/subdir/sub_big_file.bin', 'wb') as f:\n",
    "#     f.write(os.urandom(1000000))    # generate random content file larger than ROW_SIZE in subdir\n",
    "# crc1 = crc32(f'{tmp_entities.tmp_dir_to_store}/subdir/sub_big_file.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9ae426c-169f-44c2-98b5-9ce51c02ee6f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sinara.archive import SinaraArchive\n",
    "arhive = SinaraArchive(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6462c15a-8ca0-4e2c-9840-702c19029a2b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 100000; count: 50000; time: 18.785607662051916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 100000; count: 50000; time: 12.21726895030588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 10000000; count: 500; time: 25.16897779982537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 100000000; count: 50; time: 9.512062037363648\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os, string, time, random\n",
    "from pathlib import Path\n",
    "\n",
    "tmp_entities = substep.tmp_entities()\n",
    "outputs = substep.outputs()\n",
    "\n",
    "stored_files = [os.path.basename(x) for x in glob.glob(tmp_entities.tmp_dir_to_store + '/**', recursive=True)]\n",
    "print(stored_files)\n",
    "\n",
    "total_size = 5000000000\n",
    "arhive.BLOCK_SIZE = 100 * 1024 * 1024\n",
    "#arhive.ROW_SIZE = 100 * 1024\n",
    "\n",
    "for file_size in [100000, 100000, 10000000, 100000000]:\n",
    "    files_count = int(total_size / file_size)\n",
    "    for i in range(1, files_count):\n",
    "        with open(f'{tmp_entities.tmp_dir_to_store}/big_file_{i}.bin', 'wb') as f:\n",
    "            f.write(os.urandom(file_size))\n",
    "\n",
    "    t_start = time.perf_counter()\n",
    "    arhive.pack_files_from_tmp_to_store(tmp_entities.tmp_dir_to_store, outputs.stored_files)\n",
    "    all_time = time.perf_counter() - t_start\n",
    "    \n",
    "    print(f\"size: {file_size}; count: {files_count}; time: {all_time}\")\n",
    "    \n",
    "    rm_rf(tmp_entities.tmp_dir_to_store)\n",
    "    rm_rf(outputs.stored_files)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b2722c8-f787-4e82-b597-0fc806f1deb1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SinaraSpark.stop_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2323c479-4510-404d-9aff-f615c64426f1",
   "metadata": {},
   "source": [
    "# 10\n",
    "```\n",
    "24/06/11 08:15:24 WARN SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.\n",
    "size: 10000; count: 500000; time: 54.528294269926846\n",
    "size: 10000; count: 500000; time: 50.58823824673891\n",
    "size: 100000; count: 50000; time: 10.615476909093559\n",
    "size: 10000000; count: 500; time: 10.243693131022155\n",
    "size: 100000000; count: 50; time: 9.453986265696585\n",
    "\n",
    "size: 10000; count: 500000; time: 54.31516759702936\n",
    "size: 10000; count: 500000; time: 51.02222422324121\n",
    "size: 100000; count: 50000; time: 9.990651411004364\n",
    "size: 10000000; count: 500; time: 9.846800909843296\n",
    "size: 100000000; count: 50; time: 9.265271169133484\n",
    "\n",
    "size: 100000; count: 50000; time: 16.17284558620304\n",
    "24/06/11 08:52:51 WARN SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.\n",
    "size: 100000; count: 50000; time: 9.992413585074246\n",
    "size: 10000000; count: 500; time: 9.9931136877276\n",
    "size: 100000000; count: 50; time: 9.151462461333722\n",
    "```\n",
    "\n",
    "# 20\n",
    "```\n",
    "size: 100000; count: 50000; time: 15.445336972828954\n",
    "size: 100000; count: 50000; time: 9.1085306070745\n",
    "size: 10000000; count: 500; time: 10.776335041038692\n",
    "size: 100000000; count: 50; time: 8.808542616665363\n",
    "\n",
    "24/06/11 09:09:24 WARN SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.\n",
    "size: 10000; count: 500000; time: 53.49461039388552\n",
    "size: 10000; count: 500000; time: 49.78237945586443\n",
    "size: 100000; count: 50000; time: 9.234182722866535\n",
    "size: 100000; count: 50000; time: 8.735903637949377\n",
    "size: 10000000; count: 500; time: 10.782224525231868\n",
    "size: 100000000; count: 50; time: 8.998356034047902\n",
    "```\n",
    "\n",
    "# 50\n",
    "```\n",
    "24/06/11 09:17:50 WARN SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.\n",
    "size: 10000; count: 500000; time: 68.61532019311562\n",
    "size: 10000; count: 500000; time: 65.46855010604486\n",
    "size: 100000; count: 50000; time: 11.48712065583095\n",
    "size: 100000; count: 50000; time: 10.554944942239672\n",
    "size: 10000000; count: 500; time: 15.843907570000738\n",
    "\n",
    "```\n",
    "\n",
    "# 100\n",
    "```\n",
    "24/06/11 09:24:00 WARN SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.\n",
    "size: 10000; count: 500000; time: 68.16287109581754\n",
    "size: 10000; count: 500000; time: 65.88318243110552\n",
    "size: 100000; count: 50000; time: 12.077333530876786\n",
    "size: 100000; count: 50000; time: 10.022179109975696\n",
    "size: 10000000; count: 500; time: 25.557263953145593\n",
    "size: 100000000; count: 50; time: 9.462536289356649\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c24272-8475-4124-ae74-071d9d9000ca",
   "metadata": {},
   "source": [
    "# 10\n",
    "BLOCK_SIZE = 10 * 1024 * 1024 - defines number of partitions</br>\n",
    "ROW_SIZE = 100 * 1024 - defines files chunk size\n",
    "```\n",
    "size: 1000000; count: 5000; time: 54.4906174539974\n",
    "size: 1000000; count: 5000; time: 46.96132314600254\n",
    "size: 10000000; count: 500; time: 65.37026316500123\n",
    "size: 100000000; count: 50; time: 67.36531332600134\n",
    "\n",
    "size: 100000; count: 50000; time: 59.05600920800134\n",
    "size: 1000000; count: 5000; time: 48.675525227001344\n",
    "size: 10000000; count: 500; time: 66.57178226700125\n",
    "size: 100000000; count: 50; time: 66.9379985839987\n",
    "\n",
    "size: 100000; count: 50000; time: 57.292460402997676\n",
    "size: 100000; count: 50000; time: 54.66207834600209\n",
    "size: 10000000; count: 500; time: 68.61515304999921\n",
    "size: 100000000; count: 50; time: 64.979802793001\n",
    "\n",
    "\n",
    "24/06/10 15:51:49 WARN SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.\n",
    "size: 10000; count: 500000; time: 161.07436843399773\n",
    "size: 10000; count: 500000; time: 142.60217995700077\n",
    "size: 100000; count: 50000; time: 54.30553206100012\n",
    "size: 10000000; count: 500; time: 65.6778254469973\n",
    "size: 100000000; count: 50; time: 62.57982315299887\n",
    "```\n",
    "\n",
    "# 20\n",
    "arhive.BLOCK_SIZE = 20 * 1024 * 1024</br>\n",
    "arhive.ROW_SIZE = 100 * 1024</br>\n",
    "24/06/10 15:14:08 WARN SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.\n",
    "```\n",
    "size: 10000; count: 500000; time: 150.41656158399564\n",
    "size: 10000; count: 500000; time: 144.28740993400424\n",
    "size: 100000; count: 50000; time: 51.9730229850029\n",
    "size: 10000000; count: 500; time: 68.41051908200461\n",
    "size: 100000000; count: 50; time: 63.28373109500535\n",
    "```\n",
    "\n",
    "# 30\n",
    "BLOCK_SIZE = 30 * 1024 * 1024</br>\n",
    "ROW_SIZE = 100 * 1024</br>\n",
    "24/06/10 15:14:08 WARN SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.\n",
    "```\n",
    "size: 10000; count: 500000; time: 156.74156803900405\n",
    "size: 10000; count: 500000; time: 165.01108286800445\n",
    "size: 100000; count: 50000; time: 51.56755971300299\n",
    "size: 10000000; count: 500; time: 70.55579972799751\n",
    "size: 100000000; count: 50; time: 64.03572041999723\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6612d30b-fb8e-4385-9efe-4eb920160839",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
