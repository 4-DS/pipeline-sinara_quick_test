{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "870a1090-f4ae-45a0-aa93-0960f5c6478d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "453c78e8-6822-4123-bf9d-ed1ae9dd7bea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# specify substep parameters for interactive run\n",
    "# this cell will be replaced during job run with the parameters from json within params subfolder\n",
    "substep_params={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50592002-8b5f-47bd-82a7-3489fd1f5b1c",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Pipeline params:**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X': 'something',\n",
      " 'env_name': 'user',\n",
      " 'pipeline_name': 'pipeline',\n",
      " 'zone_name': 'zone'}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Step params:**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Y': 'something_else'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load pipeline and step parameters - do not edit\n",
    "from sinara.substep import get_pipeline_params, get_step_params\n",
    "pipeline_params = get_pipeline_params(pprint=True)\n",
    "step_params = get_step_params(pprint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3818420-6085-43ea-b97e-578742794e1e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "interface"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**STEP NAME:**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'sinara_quick_test'\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**TMP OUTPUTS:**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'tmp:user.pipeline.zone.sinara_quick_test.test_data': '/tmp/env/user/pipeline/zone/sinara_quick_test/run-24-08-12-072026/test_data'}]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3 define substep interface\n",
    "from sinara.substep import NotebookSubstep, ENV_NAME, PIPELINE_NAME, ZONE_NAME, STEP_NAME, RUN_ID, ENTITY_NAME, ENTITY_PATH, SUBSTEP_NAME\n",
    "\n",
    "substep = NotebookSubstep(pipeline_params, step_params, substep_params)\n",
    "\n",
    "substep.interface(\n",
    "   \n",
    "    tmp_outputs =\n",
    "    [\n",
    "        { ENTITY_NAME: \"test_data\" },\n",
    "    ]\n",
    ")\n",
    "\n",
    "substep.print_interface_info()\n",
    "\n",
    "substep.exit_in_visualize_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55e950c3-3151-4daf-9b98-83d3aebe216d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session is run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/12 07:20:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='/proxy/4040/jobs/' target='blank'>Open Spark UI</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 34698)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sinara.spark import SinaraSpark\n",
    "\n",
    "spark = SinaraSpark.run_session(0)\n",
    "SinaraSpark.ui_url()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b791393-abc4-4e8d-981d-051c89c064ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd29eecc-d0ba-4062-8b1e-76b772589a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "981628ad-3838-4184-b59b-2e4ae621ca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_outputs = substep.tmp_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d684977e-b2ea-4752-a866-e26268257f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20c2baac-752d-4a47-b27e-ec97e632149f",
   "metadata": {},
   "source": [
    "# Генерация данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bc1a5d5-e69d-4493-a46d-86acde5bd69b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f4d57aedb644df8d1c2ea9c82c5ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/12 07:21:55 WARN BaseSessionStateBuilder$$anon$2: Max iterations (100) reached for batch LocalRelation early, please set 'spark.sql.optimizer.maxIterations' to a larger value.\n",
      "24/08/12 07:22:05 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/08/12 07:22:06 WARN TaskSetManager: Stage 0 contains a task of very large size (4420 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "n_rows = 30000 #3000000\n",
    "n_cols = 200\n",
    "\n",
    "df_spark = spark.createDataFrame(np.random.sample((n_rows, 1)))\n",
    "for i in tqdm(range(n_cols-1), total=n_cols-1):\n",
    "    df_spark = df_spark.withColumn(f'value_{i}', F.col('value'))\n",
    "\n",
    "df_spark \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(tmp_outputs.test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf027629-91c4-429e-9142-c68c3b814b63",
   "metadata": {},
   "source": [
    "# Чтение данных через pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ec7188d-9266-41b1-a0f1-370eec57823a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 263 ms, sys: 556 ms, total: 819 ms\n",
      "Wall time: 245 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_pandas_1 = pd.read_parquet(tmp_outputs.test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d27b0e2a-5c8d-4571-b01a-6a8c6f57dd5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 200)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10b38609-9dc9-44b3-b6c2-a572c455a15a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>value_0</th>\n",
       "      <th>value_1</th>\n",
       "      <th>value_2</th>\n",
       "      <th>value_3</th>\n",
       "      <th>value_4</th>\n",
       "      <th>value_5</th>\n",
       "      <th>value_6</th>\n",
       "      <th>value_7</th>\n",
       "      <th>value_8</th>\n",
       "      <th>...</th>\n",
       "      <th>value_189</th>\n",
       "      <th>value_190</th>\n",
       "      <th>value_191</th>\n",
       "      <th>value_192</th>\n",
       "      <th>value_193</th>\n",
       "      <th>value_194</th>\n",
       "      <th>value_195</th>\n",
       "      <th>value_196</th>\n",
       "      <th>value_197</th>\n",
       "      <th>value_198</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.672600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.305770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.140396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.417841</td>\n",
       "      <td>0.417841</td>\n",
       "      <td>0.417841</td>\n",
       "      <td>0.417841</td>\n",
       "      <td>0.417841</td>\n",
       "      <td>0.417841</td>\n",
       "      <td>0.417841</td>\n",
       "      <td>0.417841</td>\n",
       "      <td>0.417841</td>\n",
       "      <td>0.417841</td>\n",
       "      <td>...</td>\n",
       "      <td>0.417841</td>\n",
       "      <td>0.417841</td>\n",
       "      <td>0.417841</td>\n",
       "      <td>0.417841</td>\n",
       "      <td>0.417841</td>\n",
       "      <td>0.417841</td>\n",
       "      <td>0.417841</td>\n",
       "      <td>0.417841</td>\n",
       "      <td>0.417841</td>\n",
       "      <td>0.417841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.766637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      value   value_0   value_1   value_2   value_3   value_4   value_5  \\\n",
       "0  0.672600  0.672600  0.672600  0.672600  0.672600  0.672600  0.672600   \n",
       "1  0.305770  0.305770  0.305770  0.305770  0.305770  0.305770  0.305770   \n",
       "2  0.140396  0.140396  0.140396  0.140396  0.140396  0.140396  0.140396   \n",
       "3  0.417841  0.417841  0.417841  0.417841  0.417841  0.417841  0.417841   \n",
       "4  0.766637  0.766637  0.766637  0.766637  0.766637  0.766637  0.766637   \n",
       "\n",
       "    value_6   value_7   value_8  ...  value_189  value_190  value_191  \\\n",
       "0  0.672600  0.672600  0.672600  ...   0.672600   0.672600   0.672600   \n",
       "1  0.305770  0.305770  0.305770  ...   0.305770   0.305770   0.305770   \n",
       "2  0.140396  0.140396  0.140396  ...   0.140396   0.140396   0.140396   \n",
       "3  0.417841  0.417841  0.417841  ...   0.417841   0.417841   0.417841   \n",
       "4  0.766637  0.766637  0.766637  ...   0.766637   0.766637   0.766637   \n",
       "\n",
       "   value_192  value_193  value_194  value_195  value_196  value_197  value_198  \n",
       "0   0.672600   0.672600   0.672600   0.672600   0.672600   0.672600   0.672600  \n",
       "1   0.305770   0.305770   0.305770   0.305770   0.305770   0.305770   0.305770  \n",
       "2   0.140396   0.140396   0.140396   0.140396   0.140396   0.140396   0.140396  \n",
       "3   0.417841   0.417841   0.417841   0.417841   0.417841   0.417841   0.417841  \n",
       "4   0.766637   0.766637   0.766637   0.766637   0.766637   0.766637   0.766637  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93897d2-2060-4d7c-85e5-994127792308",
   "metadata": {},
   "source": [
    "# Чтение данных через spark и преобразование в pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a5b177a-29b0-4956-bb16-6d9c82d99df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                        (0 + 11) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 85.2 ms, sys: 322 ms, total: 407 ms\n",
      "Wall time: 2.83 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_pandas_2 = spark \\\n",
    "    .read \\\n",
    "    .parquet(tmp_outputs.test_data) \\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b123cf0-1520-4da5-aa8c-3d152abbe362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>value_0</th>\n",
       "      <th>value_1</th>\n",
       "      <th>value_2</th>\n",
       "      <th>value_3</th>\n",
       "      <th>value_4</th>\n",
       "      <th>value_5</th>\n",
       "      <th>value_6</th>\n",
       "      <th>value_7</th>\n",
       "      <th>value_8</th>\n",
       "      <th>...</th>\n",
       "      <th>value_189</th>\n",
       "      <th>value_190</th>\n",
       "      <th>value_191</th>\n",
       "      <th>value_192</th>\n",
       "      <th>value_193</th>\n",
       "      <th>value_194</th>\n",
       "      <th>value_195</th>\n",
       "      <th>value_196</th>\n",
       "      <th>value_197</th>\n",
       "      <th>value_198</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.093832</td>\n",
       "      <td>0.093832</td>\n",
       "      <td>0.093832</td>\n",
       "      <td>0.093832</td>\n",
       "      <td>0.093832</td>\n",
       "      <td>0.093832</td>\n",
       "      <td>0.093832</td>\n",
       "      <td>0.093832</td>\n",
       "      <td>0.093832</td>\n",
       "      <td>0.093832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.093832</td>\n",
       "      <td>0.093832</td>\n",
       "      <td>0.093832</td>\n",
       "      <td>0.093832</td>\n",
       "      <td>0.093832</td>\n",
       "      <td>0.093832</td>\n",
       "      <td>0.093832</td>\n",
       "      <td>0.093832</td>\n",
       "      <td>0.093832</td>\n",
       "      <td>0.093832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.382171</td>\n",
       "      <td>0.382171</td>\n",
       "      <td>0.382171</td>\n",
       "      <td>0.382171</td>\n",
       "      <td>0.382171</td>\n",
       "      <td>0.382171</td>\n",
       "      <td>0.382171</td>\n",
       "      <td>0.382171</td>\n",
       "      <td>0.382171</td>\n",
       "      <td>0.382171</td>\n",
       "      <td>...</td>\n",
       "      <td>0.382171</td>\n",
       "      <td>0.382171</td>\n",
       "      <td>0.382171</td>\n",
       "      <td>0.382171</td>\n",
       "      <td>0.382171</td>\n",
       "      <td>0.382171</td>\n",
       "      <td>0.382171</td>\n",
       "      <td>0.382171</td>\n",
       "      <td>0.382171</td>\n",
       "      <td>0.382171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.357135</td>\n",
       "      <td>0.357135</td>\n",
       "      <td>0.357135</td>\n",
       "      <td>0.357135</td>\n",
       "      <td>0.357135</td>\n",
       "      <td>0.357135</td>\n",
       "      <td>0.357135</td>\n",
       "      <td>0.357135</td>\n",
       "      <td>0.357135</td>\n",
       "      <td>0.357135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.357135</td>\n",
       "      <td>0.357135</td>\n",
       "      <td>0.357135</td>\n",
       "      <td>0.357135</td>\n",
       "      <td>0.357135</td>\n",
       "      <td>0.357135</td>\n",
       "      <td>0.357135</td>\n",
       "      <td>0.357135</td>\n",
       "      <td>0.357135</td>\n",
       "      <td>0.357135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.312755</td>\n",
       "      <td>0.312755</td>\n",
       "      <td>0.312755</td>\n",
       "      <td>0.312755</td>\n",
       "      <td>0.312755</td>\n",
       "      <td>0.312755</td>\n",
       "      <td>0.312755</td>\n",
       "      <td>0.312755</td>\n",
       "      <td>0.312755</td>\n",
       "      <td>0.312755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.312755</td>\n",
       "      <td>0.312755</td>\n",
       "      <td>0.312755</td>\n",
       "      <td>0.312755</td>\n",
       "      <td>0.312755</td>\n",
       "      <td>0.312755</td>\n",
       "      <td>0.312755</td>\n",
       "      <td>0.312755</td>\n",
       "      <td>0.312755</td>\n",
       "      <td>0.312755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.687973</td>\n",
       "      <td>0.687973</td>\n",
       "      <td>0.687973</td>\n",
       "      <td>0.687973</td>\n",
       "      <td>0.687973</td>\n",
       "      <td>0.687973</td>\n",
       "      <td>0.687973</td>\n",
       "      <td>0.687973</td>\n",
       "      <td>0.687973</td>\n",
       "      <td>0.687973</td>\n",
       "      <td>...</td>\n",
       "      <td>0.687973</td>\n",
       "      <td>0.687973</td>\n",
       "      <td>0.687973</td>\n",
       "      <td>0.687973</td>\n",
       "      <td>0.687973</td>\n",
       "      <td>0.687973</td>\n",
       "      <td>0.687973</td>\n",
       "      <td>0.687973</td>\n",
       "      <td>0.687973</td>\n",
       "      <td>0.687973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      value   value_0   value_1   value_2   value_3   value_4   value_5  \\\n",
       "0  0.093832  0.093832  0.093832  0.093832  0.093832  0.093832  0.093832   \n",
       "1  0.382171  0.382171  0.382171  0.382171  0.382171  0.382171  0.382171   \n",
       "2  0.357135  0.357135  0.357135  0.357135  0.357135  0.357135  0.357135   \n",
       "3  0.312755  0.312755  0.312755  0.312755  0.312755  0.312755  0.312755   \n",
       "4  0.687973  0.687973  0.687973  0.687973  0.687973  0.687973  0.687973   \n",
       "\n",
       "    value_6   value_7   value_8  ...  value_189  value_190  value_191  \\\n",
       "0  0.093832  0.093832  0.093832  ...   0.093832   0.093832   0.093832   \n",
       "1  0.382171  0.382171  0.382171  ...   0.382171   0.382171   0.382171   \n",
       "2  0.357135  0.357135  0.357135  ...   0.357135   0.357135   0.357135   \n",
       "3  0.312755  0.312755  0.312755  ...   0.312755   0.312755   0.312755   \n",
       "4  0.687973  0.687973  0.687973  ...   0.687973   0.687973   0.687973   \n",
       "\n",
       "   value_192  value_193  value_194  value_195  value_196  value_197  value_198  \n",
       "0   0.093832   0.093832   0.093832   0.093832   0.093832   0.093832   0.093832  \n",
       "1   0.382171   0.382171   0.382171   0.382171   0.382171   0.382171   0.382171  \n",
       "2   0.357135   0.357135   0.357135   0.357135   0.357135   0.357135   0.357135  \n",
       "3   0.312755   0.312755   0.312755   0.312755   0.312755   0.312755   0.312755  \n",
       "4   0.687973   0.687973   0.687973   0.687973   0.687973   0.687973   0.687973  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e31bc0e1-40e6-4b0d-896c-e05243da59e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 200)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e611709e-a343-4e09-8867-6126113ccf4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 769 µs, sys: 30.3 ms, total: 31.1 ms\n",
      "Wall time: 31.4 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_pandas_1.equals(df_pandas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "379e08c6-078c-4d3f-85e6-49aba1483e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_pandas_1.dtypes == df_pandas_2.dtypes).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8475ea6-8200-44bc-9752-5fca0e826c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_pandas_1.values == df_pandas_2.values).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69352234-2b21-4979-9da5-9fa7865fd975",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas_3 = df_pandas_2 \\\n",
    "    .set_index('value').loc[df_pandas_1['value'].values] \\\n",
    "    .reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "204996bc-6c8f-4fa8-9729-f289cda7506d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>value_0</th>\n",
       "      <th>value_1</th>\n",
       "      <th>value_2</th>\n",
       "      <th>value_3</th>\n",
       "      <th>value_4</th>\n",
       "      <th>value_5</th>\n",
       "      <th>value_6</th>\n",
       "      <th>value_7</th>\n",
       "      <th>value_8</th>\n",
       "      <th>...</th>\n",
       "      <th>value_189</th>\n",
       "      <th>value_190</th>\n",
       "      <th>value_191</th>\n",
       "      <th>value_192</th>\n",
       "      <th>value_193</th>\n",
       "      <th>value_194</th>\n",
       "      <th>value_195</th>\n",
       "      <th>value_196</th>\n",
       "      <th>value_197</th>\n",
       "      <th>value_198</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.672600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.305770</td>\n",
       "      <td>0.305770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.140396</td>\n",
       "      <td>0.140396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.417841</td>\n",
       "      <td>0.417841</td>\n",
       "      <td>0.417841</td>\n",
       "      <td>0.417841</td>\n",
       "      <td>0.417841</td>\n",
       "      <td>0.417841</td>\n",
       "      <td>0.417841</td>\n",
       "      <td>0.417841</td>\n",
       "      <td>0.417841</td>\n",
       "      <td>0.417841</td>\n",
       "      <td>...</td>\n",
       "      <td>0.417841</td>\n",
       "      <td>0.417841</td>\n",
       "      <td>0.417841</td>\n",
       "      <td>0.417841</td>\n",
       "      <td>0.417841</td>\n",
       "      <td>0.417841</td>\n",
       "      <td>0.417841</td>\n",
       "      <td>0.417841</td>\n",
       "      <td>0.417841</td>\n",
       "      <td>0.417841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.766637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      value   value_0   value_1   value_2   value_3   value_4   value_5  \\\n",
       "0  0.672600  0.672600  0.672600  0.672600  0.672600  0.672600  0.672600   \n",
       "1  0.305770  0.305770  0.305770  0.305770  0.305770  0.305770  0.305770   \n",
       "2  0.140396  0.140396  0.140396  0.140396  0.140396  0.140396  0.140396   \n",
       "3  0.417841  0.417841  0.417841  0.417841  0.417841  0.417841  0.417841   \n",
       "4  0.766637  0.766637  0.766637  0.766637  0.766637  0.766637  0.766637   \n",
       "\n",
       "    value_6   value_7   value_8  ...  value_189  value_190  value_191  \\\n",
       "0  0.672600  0.672600  0.672600  ...   0.672600   0.672600   0.672600   \n",
       "1  0.305770  0.305770  0.305770  ...   0.305770   0.305770   0.305770   \n",
       "2  0.140396  0.140396  0.140396  ...   0.140396   0.140396   0.140396   \n",
       "3  0.417841  0.417841  0.417841  ...   0.417841   0.417841   0.417841   \n",
       "4  0.766637  0.766637  0.766637  ...   0.766637   0.766637   0.766637   \n",
       "\n",
       "   value_192  value_193  value_194  value_195  value_196  value_197  value_198  \n",
       "0   0.672600   0.672600   0.672600   0.672600   0.672600   0.672600   0.672600  \n",
       "1   0.305770   0.305770   0.305770   0.305770   0.305770   0.305770   0.305770  \n",
       "2   0.140396   0.140396   0.140396   0.140396   0.140396   0.140396   0.140396  \n",
       "3   0.417841   0.417841   0.417841   0.417841   0.417841   0.417841   0.417841  \n",
       "4   0.766637   0.766637   0.766637   0.766637   0.766637   0.766637   0.766637  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ccb636b-a270-41fc-9f5b-c1375e139d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.3 ms, sys: 653 µs, total: 20.9 ms\n",
      "Wall time: 19.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_pandas_1.equals(df_pandas_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386e2401-d1f0-48dc-af04-4f2cc9df72d0",
   "metadata": {},
   "source": [
    "# Чтение данных через spark -> pandas_api -> pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7046afe5-e9c8-406a-9208-fa9f10bc2470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n",
      "/usr/local/spark/python/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.09 s, sys: 551 ms, total: 2.64 s\n",
      "Wall time: 3.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_pandas = spark \\\n",
    "    .read \\\n",
    "    .parquet(tmp_outputs.test_data) \\\n",
    "    .pandas_api() \\\n",
    "    .to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16acb53f-f3fa-4318-a984-cda64031e31f",
   "metadata": {},
   "source": [
    "# Сохранение датафрейма pandas при помощи spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4eac9de3-2b40-408c-8114-4f5670759e15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/12 07:22:17 WARN TaskSetManager: Stage 6 contains a task of very large size (4420 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 105 ms, sys: 79.3 ms, total: 185 ms\n",
      "Wall time: 2.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# выполняется очень долго без arrow оптимизации\n",
    "spark.createDataFrame(df_pandas) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(tmp_outputs.test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beeb714-c14c-455d-b40d-e1a0f51d1ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b863c75f-d694-4f96-bd2d-6af21407d22b",
   "metadata": {},
   "source": [
    "# Уберем arrow оптимизацию"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a09a03-d9d8-4b25-ac31-6c122dcdd455",
   "metadata": {},
   "source": [
    "## Чтение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a51eb7f1-3f1e-4c91-8775-17e9b6804540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.0'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5a8874e-94a7-4d69-b496-44ef07fbbd66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.name', 'SinaraML Spark App'),\n",
       " ('spark.app.id', 'local-1723447230087'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.master', 'local[11]'),\n",
       " ('spark.driver.maxResultSize', '1g'),\n",
       " ('spark.app.submitTime', '1723447228860'),\n",
       " ('spark.driver.host', '9316bc880a1a'),\n",
       " ('spark.app.startTime', '1723447228972'),\n",
       " ('spark.driver.memory', '3g'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/home/sinarian/work/pipeline-sinara_quick_test/spark-warehouse'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true'),\n",
       " ('spark.driver.port', '45367'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.sql.execution.arrow.pyspark.enabled', 'true')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78d9783c-4a65-4d6e-98d4-a4f52b476cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', 'false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c079b1b2-6ae2-4208-a1f5-88c38884e61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.9 s, sys: 228 ms, total: 2.12 s\n",
      "Wall time: 3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_pandas_4 = spark \\\n",
    "    .read \\\n",
    "    .parquet(tmp_outputs.test_data) \\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d10d70-8e80-4243-ab71-c495a947c93b",
   "metadata": {},
   "source": [
    "## Сохранение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6aa00b41-96c9-4aa7-8089-e30f1a8ec6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/12 07:23:04 WARN TaskSetManager: Stage 9 contains a task of very large size (3613 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 9:===============>                                          (3 + 8) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.6 s, sys: 94.4 ms, total: 41.7 s\n",
      "Wall time: 45 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark.createDataFrame(df_pandas) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(tmp_outputs.test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c7e677-c907-49f6-ad5e-5ddc2b12952b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559fc4f8-7b82-42fa-b68e-d811c0fa1197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dea33a4b-11b7-47be-944b-1bb3085fc29e",
   "metadata": {},
   "source": [
    "# Эксперименты с разными типами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b7372b1-5184-4e5d-bb95-6c7842b5f979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://9316bc880a1a:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[11]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SinaraML Spark App</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f0d29a17610>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0fee7a1f-989c-41a8-88ed-85136f541573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, StringType, DataType, ArrayType, LongType, DoubleType, TimestampType\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4fe6d9-ae6c-418f-8cbc-25c6a557fc6a",
   "metadata": {},
   "source": [
    "## Генерируем данные как в MES Honeywell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e6b33ab-8256-4570-812f-b507b12abefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_row_data = 'row_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2ae3c5f-72d9-4eea-833b-bc45704a1e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(ts_start, ts_end, freq, n_tags, n_devices, path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    ts = pd.date_range(ts_start, ts_end, freq=freq)\n",
    "    ts = ts.astype(str).tolist()\n",
    "    \n",
    "    tags = [f'tag_{i}_device_{j}' for i in range(n_devices) for j in range(n_tags)]\n",
    "    for tag in tqdm(tags):\n",
    "        data = [{\n",
    "            'tag': tag,\n",
    "            'vl': list(np.random.sample(len(ts))),\n",
    "            'ts': ts,\n",
    "        }]\n",
    "        data = json.dumps(data)\n",
    "    \n",
    "        with open(f'{path}/{tag}.json', 'w') as f:\n",
    "            f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8f3bae3-5159-4052-8462-cfcd90c40f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f848c3913cad4909bfb915d4e629cf91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_generator(\n",
    "    ts_start='2023-01-01 00:00:00', \n",
    "    ts_end='2024-01-01 00:00:00', \n",
    "    freq='15s', \n",
    "    n_tags=4, \n",
    "    n_devices=2, \n",
    "    path=path_row_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d314d24-465a-42a6-9e8f-3f5ff78a721a",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('tag', StringType()), \n",
    "    StructField('ts', ArrayType(TimestampType())), \n",
    "    StructField('vl', ArrayType(DoubleType())), \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aa6cf560-4c0a-4e6e-8f47-eed0894eef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_spark = spark.read.json(path_row_data, schema=schema, multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a03f249-f854-4521-831f-b4d7231ec17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+\n",
      "|           tag|                  ts|                  vl|\n",
      "+--------------+--------------------+--------------------+\n",
      "|tag_1_device_1|[2023-01-01 00:00...|[0.94215778586690...|\n",
      "|tag_0_device_2|[2023-01-01 00:00...|[0.89420245064995...|\n",
      "|tag_1_device_0|[2023-01-01 00:00...|[0.22069186908002...|\n",
      "|tag_1_device_3|[2023-01-01 00:00...|[0.17303179917034...|\n",
      "|tag_0_device_1|[2023-01-01 00:00...|[4.12911518605008...|\n",
      "|tag_1_device_2|[2023-01-01 00:00...|[0.48173084303695...|\n",
      "|tag_0_device_3|[2023-01-01 00:00...|[0.65951874847680...|\n",
      "|tag_0_device_0|[2023-01-01 00:00...|[0.07290907681984...|\n",
      "+--------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a68c671f-f3eb-43a5-ac93-f87dbd47632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vl = F.col('tmp.vl').alias('vl')\n",
    "ts = F.col('tmp.ts').alias('ts')\n",
    "\n",
    "df_test_spark_2 = df_test_spark \\\n",
    "    .withColumn('tmp', F.arrays_zip('ts', 'vl')) \\\n",
    "    .withColumn('tmp', F.explode('tmp')) \\\n",
    "    .select('tag', vl, ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "90c2da60-0f66-4a5f-9b3f-d2fc02df38b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+-------------------+\n",
      "|           tag|                 vl|                 ts|\n",
      "+--------------+-------------------+-------------------+\n",
      "|tag_1_device_1| 0.9421577858669075|2023-01-01 00:00:00|\n",
      "|tag_1_device_1|  0.840270745087203|2023-01-01 00:00:15|\n",
      "|tag_1_device_1|0.31965728693470064|2023-01-01 00:00:30|\n",
      "|tag_1_device_1| 0.4137678799237364|2023-01-01 00:00:45|\n",
      "|tag_1_device_1| 0.9927946192378685|2023-01-01 00:01:00|\n",
      "|tag_1_device_1| 0.7339230699200996|2023-01-01 00:01:15|\n",
      "|tag_1_device_1| 0.5846028291030303|2023-01-01 00:01:30|\n",
      "|tag_1_device_1| 0.8306693181577471|2023-01-01 00:01:45|\n",
      "|tag_1_device_1| 0.5909516805792417|2023-01-01 00:02:00|\n",
      "|tag_1_device_1| 0.7403757799634568|2023-01-01 00:02:15|\n",
      "|tag_1_device_1| 0.1925497310544838|2023-01-01 00:02:30|\n",
      "|tag_1_device_1|  0.319616781456898|2023-01-01 00:02:45|\n",
      "|tag_1_device_1|  0.204129309887762|2023-01-01 00:03:00|\n",
      "|tag_1_device_1| 0.5600396858431801|2023-01-01 00:03:15|\n",
      "|tag_1_device_1|  0.858470283807323|2023-01-01 00:03:30|\n",
      "|tag_1_device_1|  0.974405862857771|2023-01-01 00:03:45|\n",
      "|tag_1_device_1| 0.4340049720225494|2023-01-01 00:04:00|\n",
      "|tag_1_device_1|  0.257142985417257|2023-01-01 00:04:15|\n",
      "|tag_1_device_1|0.37304690961964093|2023-01-01 00:04:30|\n",
      "|tag_1_device_1| 0.7294427390596602|2023-01-01 00:04:45|\n",
      "+--------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_test_spark_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd945dfe-be94-4f6d-b6bc-144939424ee7",
   "metadata": {},
   "source": [
    "## Преобразование в pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4ad85bfa-1133-4870-9fa9-037a2cc3bb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"refresh progress\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.status.AppStatusStore.activeStages(AppStatusStore.scala:169)\n",
      "\tat org.apache.spark.ui.ConsoleProgressBar.org$apache$spark$ui$ConsoleProgressBar$$refresh(ConsoleProgressBar.scala:64)\n",
      "\tat org.apache.spark.ui.ConsoleProgressBar$$anon$1.run(ConsoleProgressBar.scala:52)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "24/08/12 07:25:07 ERROR Executor: Exception in task 7.0 in stage 14.0 (TID 95)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/08/12 07:25:07 ERROR Executor: Exception in task 5.0 in stage 14.0 (TID 93)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.copy(UnsafeRow.java:492)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3818/0x0000000841546040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2604/0x00000008411a8c40.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2490/0x000000084114f440.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/08/12 07:25:07 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 7.0 in stage 14.0 (TID 95),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/08/12 07:25:07 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 5.0 in stage 14.0 (TID 93),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.copy(UnsafeRow.java:492)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3818/0x0000000841546040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2604/0x00000008411a8c40.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2490/0x000000084114f440.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/08/12 07:25:07 WARN TaskSetManager: Lost task 7.0 in stage 14.0 (TID 95) (9316bc880a1a executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "24/08/12 07:25:09 ERROR TaskSetManager: Task 7 in stage 14.0 failed 1 times; aborting job\n",
      "24/08/12 07:25:23 ERROR Executor: Exception in task 6.0 in stage 14.0 (TID 94)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/08/12 07:25:23 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 6.0 in stage 14.0 (TID 94),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/pyspark/errors/exceptions/captured.py\", line 169, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/spark/python/lib/py4j-src/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-src/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-src/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-src/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-src/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:1216\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1216\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-src/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:171\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 171\u001b[0m     converted \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_exception\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:149\u001b[0m, in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    147\u001b[0m stacktrace: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m jvm\u001b[38;5;241m.\u001b[39morg\u001b[38;5;241m.\u001b[39mapache\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mUtils\u001b[38;5;241m.\u001b[39mexceptionString(e)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m--> 149\u001b[0m     \u001b[43mis_instance_of\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.spark.api.python.PythonException\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# To make sure this only catches Python UDFs.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    153\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m v: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.sql.execution.python\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m v\u001b[38;5;241m.\u001b[39mtoString(), c\u001b[38;5;241m.\u001b[39mgetStackTrace()\n\u001b[1;32m    154\u001b[0m         )\n\u001b[1;32m    155\u001b[0m     )\n\u001b[1;32m    156\u001b[0m ):\n\u001b[1;32m    157\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  An exception was thrown from the Python worker. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see the stack trace below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m c\u001b[38;5;241m.\u001b[39mgetMessage()\n\u001b[1;32m    160\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-src/py4j/java_gateway.py:464\u001b[0m, in \u001b[0;36mis_instance_of\u001b[0;34m(gateway, java_object, java_class)\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjava_class must be a string, a JavaClass, or a JavaObject\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgateway\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpy4j\u001b[49m\u001b[38;5;241m.\u001b[39mreflection\u001b[38;5;241m.\u001b[39mTypeUtil\u001b[38;5;241m.\u001b[39misInstanceOf(\n\u001b[1;32m    465\u001b[0m     param, java_object)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-src/py4j/java_gateway.py:1725\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1723\u001b[0m message \u001b[38;5;241m=\u001b[39m compute_exception_message(\n\u001b[1;32m   1724\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m does not exist in the JVM\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name), error_message)\n\u001b[0;32m-> 1725\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(message)\n",
      "\u001b[0;31mPy4JError\u001b[0m: py4j does not exist in the JVM",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/conversion.py:208\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    209\u001b[0m column_counter \u001b[38;5;241m=\u001b[39m Counter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    211\u001b[0m corrected_dtypes: List[Optional[Type]] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:1215\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Row]:\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1197\u001b[0m \n\u001b[1;32m   1198\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;124;03m    [Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1215\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[1;32m   1216\u001b[0m         sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mcollectToPython()\n\u001b[1;32m   1217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/traceback_utils.py:81\u001b[0m, in \u001b[0;36mSCCallSiteSync.__exit__\u001b[0;34m(self, type, value, tb)\u001b[0m\n\u001b[1;32m     79\u001b[0m SCCallSiteSync\u001b[38;5;241m.\u001b[39m_spark_stack_depth \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m SCCallSiteSync\u001b[38;5;241m.\u001b[39m_spark_stack_depth \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetCallSite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-src/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-src/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-src/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-src/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-src/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_test_pandas = df_test_spark_2.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2a68a41d-0084-4a72-a284-7aef699fa283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-src/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-src/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-src/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_test_pandas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_test_pandas\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_test_pandas' is not defined"
     ]
    }
   ],
   "source": [
    "df_test_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a58f1e-43dc-4d7d-9261-142b6d89ea46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_pandas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8965d15-f9dc-4c68-b641-6127b019c960",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_pandas.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dc78f4-eda0-4aa8-8496-eb6012fe63f6",
   "metadata": {},
   "source": [
    "## Преобразование в spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee852dc6-cf97-4e50-96d3-958bb0344058",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_spark_ts = spark.createDataFrame(df_test_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf11262-488f-45e9-bc0d-477ef9fcbfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark_ts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2f5f74-4fef-4919-ba62-6c4943a5d9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark_ts.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02360cc9-fe27-435e-bc29-bef09cf9777e",
   "metadata": {},
   "source": [
    "типы данных `string`, `double`, `timestamp` корректно распознаны"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8370e9fd-7fa5-4455-9e2e-4f57e0563ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beeba98-7402-4c26-8a79-b35f03d7077b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SinaraSpark.stop_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ed2df3-7834-4e2c-9360-34201173c165",
   "metadata": {},
   "source": [
    "# Итого\n",
    "\n",
    "Конвертация датафрейма из pandas в spark и обратно занимает значительное время. Это нужно перед подачей данных в модель.  \n",
    "**Проблема заметна только при работе с большими датафреймами**\n",
    "\n",
    "Проблему можно решить двумя способами:\n",
    "- делать промежуточное сохранение на диск в parquet и считывать уже с диска при помощи pandas или spark.\n",
    "- задать параметр `spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', 'true')`\n",
    "\n",
    "Для датафрейма 3 000 000 х 200\n",
    "\n",
    "|Действие|Время, секунды|\n",
    "|-|--------|\n",
    "|Чтение parquet файлов через pandas|1|\n",
    "|Чтение parquet файлов через spark и преобразование в pandas|3|\n",
    "|Чтение parquet файлов через spark, преобразование pandas_api, преобразование в pandas|180|\n",
    "|Сохнанение pandas датафрейма при помощи spark|$\\infty$|\n",
    "|Чтение parquet файлов через spark и преобразование в pandas (arrow=True)|12|\n",
    "|Сохнанение pandas датафрейма при помощи spark (arrow=True)|35|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea8034c-12c9-4be0-92fc-dd5819f82996",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
